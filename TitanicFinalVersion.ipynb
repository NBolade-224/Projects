{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for o in [5, 25,50,75, 100, 50000]:\n",
    "# FeaturesForML = ['Age', 'Fare', 'Pclass', 'Sex','SibSp','Parch','Embarked']\n",
    "# y = Titanic1.Survived ### Prediction target\n",
    "# x = Titanic1[FeaturesForML]\n",
    "# train_X, val_X, train_y, val_y = train_test_split(x, y, random_state = 2)\n",
    "# Titanic1_model = RandomForestClassifier(max_leaf_nodes=50, random_state=1)\n",
    "# Titanic1_model.fit(train_X, train_y)\n",
    "# g = pd.get_dummies(Titanic2[FeaturesForML])\n",
    "# accuracy_score(val_y, Titanic1_model.predict(val_X))\n",
    "\n",
    "# for k in range(10):\n",
    "#     for o in [LogisticRegression(random_state=1), GaussianNB(),RandomForestClassifier(max_leaf_nodes=50, random_state=1)]:\n",
    "#         FeaturesForML = ['Age', 'Fare', 'Pclass', 'Sex']\n",
    "#         y = Titanic1.Survived ### Prediction target\n",
    "#         x = Titanic1[FeaturesForML]\n",
    "#         train_X, val_X, train_y, val_y = train_test_split(x, y, random_state = k)\n",
    "#         Titanic1_model = o\n",
    "#         Titanic1_model.fit(train_X, train_y)\n",
    "#         g = pd.get_dummies(Titanic2[FeaturesForML])\n",
    "#         accuracy_score(val_y, Titanic1_model.predict(val_X))\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# n_nodes = clf.tree_.node_count\n",
    "# children_left = clf.tree_.children_left\n",
    "# children_right = clf.tree_.children_right\n",
    "# feature = clf.tree_.feature\n",
    "# threshold = clf.tree_.threshold\n",
    "\n",
    "# node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "# is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "# stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "# while len(stack) > 0:\n",
    "#     # `pop` ensures each node is only visited once\n",
    "#     node_id, depth = stack.pop()\n",
    "#     node_depth[node_id] = depth\n",
    "\n",
    "#     # If the left and right child of a node is not the same we have a split\n",
    "#     # node\n",
    "#     is_split_node = children_left[node_id] != children_right[node_id]\n",
    "#     # If a split node, append left and right children and depth to `stack`\n",
    "#     # so we can loop through them\n",
    "#     if is_split_node:\n",
    "#         stack.append((children_left[node_id], depth + 1))\n",
    "#         stack.append((children_right[node_id], depth + 1))\n",
    "#     else:\n",
    "#         is_leaves[node_id] = True\n",
    "\n",
    "# print(\n",
    "#     \"The binary tree structure has {n} nodes and has \"\n",
    "#     \"the following tree structure:\\n\".format(n=n_nodes)\n",
    "# )\n",
    "# for i in range(n_nodes):\n",
    "#     if is_leaves[i]:\n",
    "#         print(\n",
    "#             \"{space}node={node} is a leaf node.\".format(\n",
    "#                 space=node_depth[i] * \"\\t\", node=i\n",
    "#             )\n",
    "#         )\n",
    "#     else:\n",
    "#         print(\n",
    "#             \"{space}node={node} is a split node: \"\n",
    "#             \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "#             \"else to node {right}.\".format(\n",
    "#                 space=node_depth[i] * \"\\t\",\n",
    "#                 node=i,\n",
    "#                 left=children_left[i],\n",
    "#                 feature=feature[i],\n",
    "#                 threshold=threshold[i],\n",
    "#                 right=children_right[i],\n",
    "#             )\n",
    "#         )\n",
    "# node_indicator = clf.decision_path(X_test)\n",
    "# leaf_id = clf.apply(X_test)\n",
    "\n",
    "# for fdgh in range(32,38):\n",
    "\n",
    "#     sample_id = fdgh\n",
    "#     # obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
    "#     node_index = node_indicator.indices[\n",
    "#         node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n",
    "#     ]\n",
    "\n",
    "#     print(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\n",
    "#     for node_id in node_index:\n",
    "#         # continue to the next node if it is a leaf node\n",
    "#         if leaf_id[sample_id] == node_id:\n",
    "#             continue\n",
    "\n",
    "#         # check if value of the split feature for sample 0 is below threshold\n",
    "#         if X_test[sample_id, feature[node_id]] <= threshold[node_id]:\n",
    "#             threshold_sign = \"<=\"\n",
    "#         else:\n",
    "#             threshold_sign = \">\"\n",
    "\n",
    "#         print(\n",
    "#             \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n",
    "#             \"{inequality} {threshold})\".format(\n",
    "#                 node=node_id,\n",
    "#                 sample=sample_id,\n",
    "#                 feature=feature[node_id],\n",
    "#                 value=X_test[sample_id, feature[node_id]],\n",
    "#                 inequality=threshold_sign,\n",
    "#                 threshold=threshold[node_id],\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "titanic = pd.read_csv(\"C:\\\\Users\\\\nickb\\\\Desktop\\\\Python\\\\train.csv\")\n",
    "titanic0 = pd.read_csv(\"C:\\\\Users\\\\nickb\\\\Desktop\\\\Python\\\\test.csv\")\n",
    "Titanic1 = pd.read_csv(\"C:\\\\Users\\\\nickb\\\\Desktop\\\\Python\\\\train.csv\")\n",
    "Titanic2 = pd.read_csv(\"C:\\\\Users\\\\nickb\\\\Desktop\\\\Python\\\\test.csv\")\n",
    "Titanic3 = pd.read_csv(\"C:\\\\Users\\\\nickb\\\\Desktop\\\\Python\\\\gender_submission.csv\")\n",
    "titanic.replace({'male': 1, 'female': 0}, inplace=True)\n",
    "titanic0.replace({'male': 1, 'female': 0}, inplace=True)\n",
    "titanic.replace({'S': 1, 'Q': 0, 'C' : 3}, inplace=True)\n",
    "titanic0.replace({'S': 1, 'Q': 0, 'C' : 3}, inplace=True)\n",
    "Titanic1.replace({'male': 1, 'female': 0}, inplace=True)\n",
    "Titanic2.replace({'male': 1, 'female': 0}, inplace=True)\n",
    "Titanic1.replace({'S': 1, 'Q': 0, 'C' : 3}, inplace=True)\n",
    "Titanic2.replace({'S': 1, 'Q': 0, 'C' : 3}, inplace=True)\n",
    "Titanic1.fillna(method = 'ffill', inplace = True)\n",
    "Titanic2.fillna(method = 'ffill', inplace = True)\n",
    "# Titanic1['relatives'] = Titanic1.apply (lambda row: int((row['SibSp'] + row['Parch']) > 0), axis=1)\n",
    "# Titanic2['relatives'] = Titanic2.apply (lambda row: int((row['SibSp'] + row['Parch']) > 0), axis=1)\n",
    "#girlsontitanic = titanic[(titanic['Age'] >= 16) & (titanic['Age'] <= 60) & (titanic['Sex'] == 'male') & (titanic['Pclass'] == 1)]\n",
    "# Boysontitanic = titanic[(titanic['Age'] >= 10) & (titanic['Age'] <= 16) & (titanic['Sex'] == 'male')]\n",
    "# girlsontitanic = titanic[(titanic['Age'] >= 10) & (titanic['Age'] <= 16) & (titanic['Sex'] == 'female')]\n",
    "\n",
    "\n",
    "#####split testing\n",
    "FeaturesForML = ['Sex','Age','Pclass','Fare','SibSp','Parch']\n",
    "# for x in FeaturesForML:\n",
    "#     Titanic1.corr()[x]\n",
    "y = Titanic1.Survived ### Prediction target\n",
    "X = Titanic1[FeaturesForML]\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 5545, test_size=0.4)  #test_size = 0.33\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=573478,criterion=\"gini\", min_samples_leaf=25)  # 'gini'\n",
    "gh = clf.fit(train_X, train_y)\n",
    "g = pd.get_dummies(Titanic2[FeaturesForML])\n",
    "Prediction = clf.predict(g)\n",
    "accuracy_score(val_y, clf.predict(val_X))\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "# ######non-split testing\n",
    "# FeaturesForML = ['Age','Pclass', 'Sex','relatives']\n",
    "# y = Titanic1.Survived ### Prediction target\n",
    "# X = Titanic1[FeaturesForML]\n",
    "# train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\n",
    "# clf = DecisionTreeClassifier(max_depth=3, random_state=1,criterion=\"entropy\")\n",
    "# gh = clf.fit(X, y)\n",
    "# g = pd.get_dummies(Titanic2[FeaturesForML])\n",
    "# Prediction = clf.predict(g)\n",
    "#accuracy_score(y, clf.predict(g))\n",
    "\n",
    "\n",
    "# output = pd.DataFrame({'PassengerId': Titanic2.PassengerId, 'Survived': Prediction})\n",
    "# output.to_csv('submission1.csv', index=False)\n",
    "# print(\"Your submission was successfully saved!\")\n",
    "\n",
    "# Boysontitanic['Survived'].value_counts().plot(kind = 'pie' ,autopct = '.%2f')\n",
    "# plt.show()\n",
    "\n",
    "# girlsontitanic['Survived'].value_counts().plot(kind = 'pie' ,autopct = '.%2f')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(dpi = 300)\n",
    "\n",
    "\n",
    "\n",
    "plt.show(tree.plot_tree(gh,\n",
    "          feature_names = FeaturesForML,class_names=True))\n",
    "\n",
    "\n",
    "# secondclassfemales = titanic[(titanic['Pclass'] == 3) & (titanic['Sex'] == 'female')]\n",
    "# secondclassfemales['Survived'].value_counts().plot(kind = 'pie' ,autopct = '.%2f')\n",
    "# secondclassfemales.corr()['Survived']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y = titanic.Survived ### Prediction target\n",
    "X = titanic[FeaturesForML]\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 786789, test_size=0.5)  #test_size = 0.33\n",
    "\n",
    "categorical_cols = [cname for cname in train_X.columns if train_X[cname].nunique() < 10 and train_X[cname].dtype == \"object\"]\n",
    "numerical_cols = [cname for cname in train_X.columns if train_X[cname].dtype in ['int64', 'float64']]   ### THIS REMOVES CAT COLUMNS THAT ARE WAY TOO LARGE THAT HAVE LITTLE IN COMMON, \n",
    "###                                                                                                               ### SINCE NO STATISTICAL MEANING CAN BE FOUND FROM THEM, SUCH AS UNIQUE ADDRESS WITH NO REFERENCE TO POST CODES ETC.\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = train_X[my_cols].copy()\n",
    "X_valid = val_X[my_cols].copy()\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')   ### IMPUTATAES (REPLACES MISSING VALUES)\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),('onehot', OneHotEncoder(handle_unknown='ignore'))])  ### IMPUTES AND ONEHOT ENCODES\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),('cat', categorical_transformer, categorical_cols)])   ### BUNDLES TRANSFORMERS TOGETHER\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', XGBClassifier(n_estimators=25))])\n",
    "gh = my_pipeline.fit(X_train, train_y)   \n",
    "g = pd.get_dummies(titanic0[FeaturesForML])\n",
    "Prediction = my_pipeline.predict(g)\n",
    "\n",
    "accuracy_score(val_y, my_pipeline.predict(X_valid))\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': titanic0.PassengerId, 'Survived': Prediction})\n",
    "output.to_csv('submission2.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8295964125560538\n",
      "0.7600896860986547\n",
      "0.7945347533632289\n",
      "next\n",
      "0.8295964125560538\n",
      "0.7466367713004485\n",
      "0.7974775784753366\n",
      "next\n",
      "0.820627802690583\n",
      "0.7511210762331838\n",
      "0.7834641255605381\n",
      "next\n",
      "0.8475336322869955\n",
      "0.7376681614349776\n",
      "0.7960482062780272\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "modellist = [RandomForestClassifier(max_depth=3),DecisionTreeClassifier(max_depth=3),GaussianNB(),XGBClassifier(eval_metric='logloss',use_label_encoder=False)]\n",
    "for models in modellist:\n",
    "    scores = []\n",
    "    for states in range(20,100):\n",
    "        y = titanic.Survived ### Prediction target\n",
    "        X = titanic[FeaturesForML]\n",
    "        train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = states, test_size=0.5)  #test_size = 0.33\n",
    "        categorical_cols = [cname for cname in train_X.columns if train_X[cname].nunique() < 10 and train_X[cname].dtype == \"object\"]\n",
    "        numerical_cols = [cname for cname in train_X.columns if train_X[cname].dtype in ['int64', 'float64']]   ### THIS REMOVES CAT COLUMNS THAT ARE WAY TOO LARGE THAT HAVE LITTLE IN COMMON,                                                                                                            ### SINCE NO STATISTICAL MEANING CAN BE FOUND FROM THEM, SUCH AS UNIQUE ADDRESS WITH NO REFERENCE TO POST CODES ETC.\n",
    "        my_cols = categorical_cols + numerical_cols\n",
    "        X_train = train_X[my_cols].copy()\n",
    "        X_valid = val_X[my_cols].copy()\n",
    "        numerical_transformer = SimpleImputer(strategy='constant')   ### IMPUTATAES (REPLACES MISSING VALUES)\n",
    "        categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),('onehot', OneHotEncoder(handle_unknown='ignore'))])  ### IMPUTES AND ONEHOT ENCODES\n",
    "        preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),('cat', categorical_transformer, categorical_cols)])   ### BUNDLES TRANSFORMERS TOGETHER\n",
    "        my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', models)])\n",
    "        gh = my_pipeline.fit(X_train, train_y)   \n",
    "        g = pd.get_dummies(titanic0[FeaturesForML])\n",
    "        Prediction = my_pipeline.predict(g)\n",
    "        scores.append(accuracy_score(val_y, my_pipeline.predict(X_valid))) \n",
    "    mean = sum(scores) / len(scores)\n",
    "    print(max(scores))\n",
    "    print(min(scores))\n",
    "    print(mean)\n",
    "    print('next')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # secondclassfemales = titanic[(titanic['Pclass'] == 3) & (titanic['Sex'] == 'female')]\n",
    "# # secondclassfemales2 = titanic[(titanic['Pclass'] == 3) & (titanic['Sex'] == 'female') & (titanic['Fare'] > 6) & (titanic['Fare'] < 21 )]\n",
    "\n",
    "# Boys = Titanic1[(Titanic1['Sex'] == 0) & (Titanic1['Age'] < 12 )]\n",
    "\n",
    "# Boys['Survived'].value_counts().plot(kind = 'pie' ,autopct = '.%2f')\n",
    "# Boys.corr()['Survived']\n",
    "\n",
    "\n",
    "# sns.displot(x = Boys['Age'], kde = True)\n",
    "# plt.show()\n",
    "# plt.clf()\n",
    "# #sns.kdeplot(x = titanic['Age'])\n",
    "\n",
    "# not_survived_age = Boys['Survived'] == 0\n",
    "# survived_age = Boys['Survived'] == 1\n",
    "# sns.kdeplot(Boys[not_survived_age]['Fare'])\n",
    "# sns.kdeplot(Boys[survived_age]['Fare'])\n",
    "\n",
    "# plt.show()\n",
    "# plt.clf()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86b62185bc97c39efc21655c4fe9442102aed1230b4709b1ebf59ac10e900490"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
